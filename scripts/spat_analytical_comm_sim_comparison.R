## Purpose: to compare the analytical probability models for the spatial
## abundance distribution with that generated by the community simulator

setwd('~/maxent/spat/scripts')

source('./spat_analytical_prob_funcs.R')
dyn.load('./heap.so')
## generate some comparitive frequency distributions
## example from pg 96 fig 4.1
#pdf('../figs/analytical_prob_funcs.pdf',width=14,height=7)
par(mfrow=c(1,2))
n0 = 112
n = 0:5
A0 = 64
A = 1
out = matrix(NA,nrow=4,ncol=length(n))
out[1,] = binomial_prob(n,A,n0,A0)
out[2,] = laplace_prob(n,A,n0,A0)
out[3,] = sapply(n, function(x) heap_prob(x,A,n0,A0, use_c=TRUE))
out[4,] = neg_bin_prob(n,A,n0,A0)

system('python ./spat_community_generation.py 1 112 500 6 False None S1_N112 &')
comms = read.csv('../comms/simulated_comms_S1_N112_C500_B6_grid.txt')
comms = as.matrix(comms)
## reshape for ease of analysis
Ncomms = 500
Nquads = 64

commMat = matrix(comms[,4],ncol=Ncomms,nrow=Nquads)
filler = matrix(rep(0:max(commMat),each=ncol(commMat)),nrow=max(commMat)+1,byrow=TRUE)
cnts = apply(rbind(commMat,filler),2,table) - 1
freqs = cnts/nrow(commMat)
freqAvg = apply(freqs,1,mean)

plot(n,out[1,],ylim=range(out,na.rm=TRUE),type='n',ylab='Probabiliy',
     main=paste('No = ',no,', A = Ao/',Ao,sep=''))
for(i in 1:3)
  lines(n,out[i,],col=i,type='l',lwd=2)
points(n,out[4,],col=4,pch=19,cex=1.5)
points(0:5,freqAvg[1:6],cex=1.5) ## Matches perfectly with HEAP from Fig 4.1
legend('topright',c('bin','lap','heap','negbin(k=1)','simulator'),
       col=c(1:4,1),lwd=c(rep(3,3),rep(NA,2)),lty=c(rep(1,3),rep(NA,2)),
       pch=c(rep(NA,3),19,1),cex=2,bty='n')
## lap, mete and negbi are equivalent
## heap and meteiter are equivalent

n0 = 5
n = 0:5
A0 = 4
A = 1
out = matrix(NA,nrow=4,ncol=length(n))
out[1,] = binomial_prob(n,A,n0,A0)
out[2,] = laplace_prob(n,A,n0,A0)
out[3,] = sapply(n, function(x) heap_prob(x,A,n0,A0, use_c=TRUE))
out[4,] = neg_bin_prob(n,A,n0,A0)

system('python ./spat_community_generation.py 1 5 500 2 False None S1_N5 &')
comms = read.csv('../comms/simulated_comms_S1_N5_C500_B2_grid.txt')
comms = as.matrix(comms)
## reshape for ease of analysis
Ncomms = 500
Nquads = 4

commMat = matrix(comms[,4],ncol=Ncomms,nrow=Nquads)
filler = matrix(rep(0:max(commMat),each=ncol(commMat)),nrow=max(commMat)+1,byrow=TRUE)
cnts = apply(rbind(commMat,filler),2,table) - 1
freqs = cnts/nrow(commMat)
freqAvg = apply(freqs,1,mean)

plot(n,out[1,],ylim=range(out,na.rm=TRUE),type='n',ylab='Probability',
     main='No = 5, A = Ao/4')
for(i in 1:3)
  lines(n,out[i,],col=i,type='l',lwd=2)
points(n,out[4,],col=4,pch=19,cex=1.5)
points(0:5,freqAvg[1:6],cex=1.5) ## Matches perfectly with HEAP from Fig 4.1
legend('topright',c('bin','lap','heap','negbin(k=1)','simulator'),
       col=c(1:4,1),lwd=c(rep(3,3),rep(NA,2)),lty=c(rep(1,3),rep(NA,2)),
       pch=c(rep(NA,3),19,1),cex=2,bty='n')
## mete and negbi are equivalent
## heap and meteiter are equivalent
#dev.off()

## Examine recursion relationships given for HEAP turnover-----------------------
## Figure 6.8 in Harte 2007 displays a linear relationship between log2 CHI and j
## examine if the recursion relations return this relationship
i = 8
j = 2:5
n0 = 10
chi = sapply(j, function(x) chi_heap(i, x, n0, use_c=TRUE))
plot(j, log2(chi), type='o')
summary(lm(log2(chi) ~ j))
## these definately display a log-linear relationship as depicted in the book 
## chapter

## Now examine the relationship depicted in Figure 6.7 in Harte 2007 if we can
## recreate this figure it is a good check on the algorithims
## first generate results using python (b/c dictionaries are necessary)
system('python spat_chi_heap_examination.py')

chi_dat = read.csv('../sorensen/harte_2007_chi_heap_results.txt')

plot(chi_appr ~ chi, data= chi_dat)
mod = lm(chi_appr ~ chi, data= chi_dat)
abline(mod)
summary(mod)

## that doesn't look quite correct, but multiplying all the values by 2 makes
## it look pretty close to the book chapter

plot(chi_appr ~ chi, data= 2 * chi_dat)
mod = lm(chi_appr ~ chi, data=2 * chi_dat)
abline(mod)
summary(mod)

## this indicates that we have a functioning analytical metric for the binary
## sorensen index. Given the proximity of the quantiative and binary sorensen DDR
## this will likely give us a good approximation of the simulated DDR once it
## is considered in relation to degree of seperation rather than geographic 
## distance

library(vegan)
source('./spat_sim_vario_func.R')

S = 20
abu = matrix(rep(200, S), ncol=S)
write.table(abu, file='../tst_abu.csv', sep=',',
            row.names=FALSE, col.names=FALSE)

system('python ./spat_community_generation.py 20 100 500 8 False ../tst_abu.csv S20_N100 &')
comms = read.csv('../comms/simulated_comms_S20_N100_empirSAD_C500_B8_grid.txt')
comms = as.matrix(comms)
system('python spat_heap_ddr.py 1 256 sqr ../tst_abu.csv ../tst_heap_ddr.csv')
heap_ddr = read.csv('../tst_heap_ddr.csv')

for(i in 1:500){
  tmp_comm = comms[comms[,1] == i, ] 
  sor_dist = 1 - vegdist(tmp_comm[ , -(1:3)])  
  coords = get_bisect_coords(8)

}


coords = get_bisect_coords(3)
bc = as.character(coords[,3])
bc1 = as.numeric(substr(bc, 1, 1))
bc2 = as.numeric(substr(bc, 2, 2))
bc3 = as.numeric(substr(bc, 3, 3))

## for diffs at j = 3
## must have same bisection for 1 and 2, ensured diff for 3
gd_mat = (as.matrix(dist(cbind(bc1, bc2))) == 0) * 1
tr_mat = gd_mat * lower.tri(gd_mat)
D_mat = as.matrix(dist(coords[, 1:2])) * tr_mat
mean(D_mat[D_mat > 0])
calc_D(3, W=2, rect=T, LW_ratio =2)

## above looks good
## below still not quite working
## for diffs at j = 2
## must have different bisection number 
gd_mat1 = (as.matrix(dist(bc1)) == 0) * 1
gd_mat2 = (as.matrix(dist(bc2)) == 1) * 1
gd_mat = gd_mat1 * gd_mat2
tr_mat = gd_mat * lower.tri(gd_mat)
D_mat = as.matrix(dist(coords[, 1:2])) * tr_mat
mean(D_mat[D_mat > 0])
calc_D(2, W=2, rect=T, LW_ratio =2)

## above is still not quite correct
## does it work for square shaped A0's? 
coords = get_bisect_coords(4)
bc = as.character(coords[,3])
bc1 = as.numeric(substr(bc, 1, 1))
bc2 = as.numeric(substr(bc, 2, 2))
bc3 = as.numeric(substr(bc, 3, 3))
bc4 = as.numeric(substr(bc, 4, 4))

## for diffs at j = 4
gd_mat = (as.matrix(dist(cbind(bc1, bc2, bc3))) == 0) * 1
tr_mat = gd_mat * lower.tri(gd_mat)
D_mat = as.matrix(dist(coords[, 1:2])) * tr_mat
mean(D_mat[D_mat > 0])
calc_D(4, W=4)

## for diffs at j = 2
gd_mat1 = (as.matrix(dist(bc1)) == 0) * 1
gd_mat2 = (as.matrix(dist(bc2)) == 1) * 1
gd_mat = gd_mat1 * gd_mat2
tr_mat = gd_mat * lower.tri(gd_mat)
D_mat = as.matrix(dist(coords[, 1:2])) * tr_mat
mean(D_mat[D_mat > 0])
calc_D(2, W=4)







gd_mat = (as.matrix(dist(bc1) + dist(bc2)) == 0) * 1
matrix(0, ncol=8, nrow=8) + gd_mat[lower.tri(gd_mat)]

tr_mat = gd_mat * lower.tri(gd_mat)
as.matrix(dist(coords[, 1:2])) * tr_mat
calc_D(2, 2)

gd_mat = (as.matrix(dist(bc1))  == 0) * 1
tr_mat = gd_mat * lower.tri(gd_mat)
d = as.matrix(dist(coords[, 1:2])) * tr_mat
mean(d[d>0])
calc_D(1, 2)

### checking chi_heap calculation
i = 2
j = 1
n0 = 3
#
chi_heap(i, j, n0)
#
(n0 + 1) ^ -1 * (calc_lambda(1, 1) * calc_lambda(1, 2) +
                 calc_lambda(1, 2) * calc_lambda(1, 1))
#
(n0 + 1)^-1 * 
  ((1 - ((1 + 1)^-1)) * (1 - ((2 + 1)^-1))) * 2
## those all match
i = 2
j = 2
n0 = 3
chi_heap(i, j, n0)
(n0 + 1) ^ -1 * 
   ( (2 + 1) ^-1 *
       (calc_lambda(0, 1) * calc_lambda(0, 1))
     +
     (3 + 1) ^-1 * 
       (calc_lambda(0, 1) * calc_lambda(0, 2) + 
        calc_lambda(0, 2) * calc_lambda(0, 1))
    )

(n0 + 1) ^-1 * ((2 + 1)^-1 + 2 * (3 + 1)^-1)
## those match too
## from first principles if i = 2 and j = 2
     ##prob of a 2 and 1 bisection
2 * ((3 + 1) ^-1  *
    ((2 + 1)^-1 * (2 + 1)^-1)) + 
2 * ((3 + 1) ^ -1 *
    2 * ((3 + 1)^-1 * (3 + 1)^-1))
chi_heap(2, 2, 3)

## for j = 2








                 




